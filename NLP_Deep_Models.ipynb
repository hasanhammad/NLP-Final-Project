{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3QaDgoQe0tQ"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import logging, AutoTokenizer, AutoModel\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the data**"
      ],
      "metadata": {
        "id": "n6M5o1qsUC5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu7fVQuRrAZM"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Copy of train_data.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Copy of test_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0GlQM6xey1S"
      },
      "outputs": [],
      "source": [
        "# Make MyDataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, sentences, labels):\n",
        "        # Initialize the MyDataset class with input sentences, labels, method_name, and model_name\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "\n",
        "        # Create a list to hold the tokenized dataset\n",
        "        dataset = list()\n",
        "        index = 0\n",
        "\n",
        "        # Iterate through each data point (sentence) in the input sentences\n",
        "        for data in sentences:\n",
        "            # Tokenize the data into individual tokens\n",
        "            tokens = data.split(' ')\n",
        "\n",
        "            # Retrieve the corresponding label for the current data point\n",
        "            labels_id = labels[index]\n",
        "\n",
        "            # Increment the index for the next iteration\n",
        "            index += 1\n",
        "\n",
        "            # Append a tuple containing tokens and labels_id to the dataset list\n",
        "            dataset.append((tokens, labels_id))\n",
        "\n",
        "        # Store the tokenized dataset internally\n",
        "        self._dataset = dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieve and return the data point (tokens, labels_id) at the given index\n",
        "        return self._dataset[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of data points in the dataset\n",
        "        return len(self.sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ1LrmTBev3C"
      },
      "outputs": [],
      "source": [
        "# Make tokens for every batch\n",
        "def my_collate(batch, tokenizer):\n",
        "    # Extract tokens and label_ids from the input batch\n",
        "    tokens, label_ids = map(list, zip(*batch))\n",
        "\n",
        "    # Tokenize the input tokens using the provided tokenizer\n",
        "    text_ids = tokenizer(tokens,\n",
        "                         padding=True,\n",
        "                         truncation=True,\n",
        "                         max_length=320,\n",
        "                         is_split_into_words=True,\n",
        "                         add_special_tokens=True,\n",
        "                         return_tensors='pt')\n",
        "\n",
        "    # Return the tokenized text_ids and corresponding label_ids as PyTorch tensors\n",
        "    return text_ids, torch.tensor(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLZuZHXncII9"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "def load_dataset(tokenizer, train_batch_size, test_batch_size):\n",
        "    # Read data from 'datasets.csv' file using pandas\n",
        "    data = pd.read_csv('/content/drive/MyDrive/datasets.csv', sep=None, header=0, encoding='utf-8', engine='python')\n",
        "\n",
        "    # Take a subset (10%) of the data for faster testing, assuming 'labels' and 'sentences' columns exist\n",
        "    len1 = int(len(list(data['labels'])) * 0.1)\n",
        "    labels = list(data['labels'])[0:len1]\n",
        "    sentences = list(data['sentences'])[0:len1]\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    train_sen, test_sen, train_lab, test_lab = train_test_split(sentences, labels, train_size=0.8)\n",
        "\n",
        "    # Create MyDataset instances for training and testing\n",
        "    train_set = MyDataset(train_sen, train_lab)\n",
        "    test_set = MyDataset(test_sen, test_lab)\n",
        "\n",
        "    # Create DataLoader instances for training and testing\n",
        "    collate_fn = partial(my_collate, tokenizer=tokenizer)\n",
        "    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, num_workers=0,\n",
        "                              collate_fn=collate_fn, pin_memory=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True, num_workers=0,\n",
        "                             collate_fn=collate_fn, pin_memory=True)\n",
        "\n",
        "    # Return the created DataLoader instances for training and testing\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihltwc15gDCe"
      },
      "outputs": [],
      "source": [
        "# FNN\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, base_model, num_classes, input_size):\n",
        "        # Initialize the Transformer class\n",
        "        super().__init__()\n",
        "\n",
        "        # Set the base model (BERT or other transformer model)\n",
        "        self.base_model = base_model\n",
        "\n",
        "        # Number of output classes\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Input size (dimensionality of input features)\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.linear = nn.Linear(base_model.config.hidden_size, num_classes)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Softmax activation function for probability distribution\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "        # Set requires_grad to True for fine-tuning the base model\n",
        "        for param in base_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Forward pass through the transformer base model\n",
        "        raw_outputs = self.base_model(**inputs)\n",
        "\n",
        "        # Extract the classification features from the last hidden state\n",
        "        cls_feats = raw_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        cls_feats_dropout = self.dropout(cls_feats)\n",
        "\n",
        "        # Apply linear layer for classification\n",
        "        predicts = self.softmax(self.linear(cls_feats_dropout))\n",
        "\n",
        "        # Return the predicted probabilities\n",
        "        return predicts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTtYJNZLhKz1"
      },
      "outputs": [],
      "source": [
        "# Bidirectional LSTM Model\n",
        "class BiLstm_Model(nn.Module):\n",
        "    def __init__(self, base_model, num_classes, input_size):\n",
        "        # Initialize the BiLstm_Model class\n",
        "        super().__init__()\n",
        "\n",
        "        # Set the base model (e.g., BERT) as the embedding layer\n",
        "        self.base_model = base_model\n",
        "\n",
        "        # Number of output classes for classification\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Input size (dimensionality of input features)\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.BiLstm = nn.LSTM(input_size=self.input_size,\n",
        "                              hidden_size=320,\n",
        "                              num_layers=1,\n",
        "                              batch_first=True,\n",
        "                              bidirectional=True)\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(320 * 2, 80),\n",
        "            nn.Linear(80, 20),\n",
        "            nn.Linear(20, self.num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Set requires_grad to True for fine-tuning the base model\n",
        "        for param in base_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Forward pass through the base model (e.g., BERT)\n",
        "        raw_outputs = self.base_model(**inputs)\n",
        "\n",
        "        # Extract the hidden states from the last layer of the base model\n",
        "        cls_feats = raw_outputs.last_hidden_state\n",
        "\n",
        "        # Apply bidirectional LSTM to the hidden states\n",
        "        outputs, _ = self.BiLstm(cls_feats)\n",
        "\n",
        "        # Select the last time step's output from the LSTM sequence\n",
        "        outputs = outputs[:, -1, :]\n",
        "\n",
        "        # Forward pass through the fully connected layers for classification\n",
        "        outputs = self.fc(outputs)\n",
        "\n",
        "        # Return the final outputs\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgFe0RgshdFe"
      },
      "outputs": [],
      "source": [
        "# TextCNN Model\n",
        "class TextCNN_Model(nn.Module):\n",
        "    def __init__(self, base_model, num_classes):\n",
        "        # Initialize the TextCNN_Model class\n",
        "        super().__init__()\n",
        "\n",
        "        # Set the base model (e.g., BERT) as the embedding layer\n",
        "        self.base_model = base_model\n",
        "\n",
        "        # Number of output classes for classification\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Set requires_grad to True for fine-tuning the base model\n",
        "        for param in base_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Hyperparameters for TextCNN\n",
        "        self.filter_sizes = [2, 3, 4]\n",
        "        self.num_filters = 2\n",
        "        self.encode_layer = 12\n",
        "\n",
        "        # Define Convolutional Layers\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv2d(in_channels=1, out_channels=self.num_filters,\n",
        "                       kernel_size=(K, self.base_model.config.hidden_size)) for K in self.filter_sizes]\n",
        "        )\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.num_filters * len(self.filter_sizes), self.num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def conv_pool(self, tokens, conv):\n",
        "        # Apply convolution, activation, squeeze, and max pooling\n",
        "        tokens = conv(tokens)\n",
        "        tokens = F.relu(tokens)\n",
        "        tokens = tokens.squeeze(3)\n",
        "        tokens = F.max_pool1d(tokens, tokens.size(2))\n",
        "        out = tokens.squeeze(2)\n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Forward pass through the base model (e.g., BERT)\n",
        "        raw_outputs = self.base_model(**inputs)\n",
        "\n",
        "        # Extract the hidden states from the last layer of the base model\n",
        "        tokens = raw_outputs.last_hidden_state.unsqueeze(1)\n",
        "\n",
        "        # Apply convolutional layers and pooling\n",
        "        out = torch.cat([self.conv_pool(tokens, conv) for conv in self.convs], 1)\n",
        "\n",
        "        # Forward pass through the fully connected layers for classification\n",
        "        predicts = self.block(out)\n",
        "\n",
        "        # Return the final predicted probabilities\n",
        "        return predicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03S6xu1Wf7ng"
      },
      "outputs": [],
      "source": [
        "# NLP Method Class\n",
        "class NLPMethod:\n",
        "    def __init__(self, method_name, train_batch_size, test_batch_size, num_epoch, lr, weight_decay):\n",
        "        # Initialization of NLPMethod class\n",
        "\n",
        "        # Model and Training Parameters\n",
        "        self.model_name = 'bert'\n",
        "        self.method_name = method_name\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = 'cuda'\n",
        "\n",
        "        # Create BERT tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.input_size = 768  # BERT hidden size\n",
        "        base_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize model based on the selected method\n",
        "        if method_name == 'fnn':\n",
        "            self.Mymodel = Transformer(base_model, 2, self.input_size)\n",
        "        elif method_name == 'bilstm':\n",
        "            self.Mymodel = BiLstm_Model(base_model, 2, self.input_size)\n",
        "        elif method_name == 'textcnn':\n",
        "            self.Mymodel = TextCNN_Model(base_model, 2)\n",
        "\n",
        "        # Move the model to the specified device\n",
        "        self.Mymodel.to(self.device)\n",
        "\n",
        "    def _train(self, dataloader, criterion, optimizer):\n",
        "        # Training function\n",
        "\n",
        "        train_loss, n_correct, n_train = 0, 0, 0\n",
        "\n",
        "        # Set the model to train mode\n",
        "        self.Mymodel.train()\n",
        "\n",
        "        for inputs, targets in tqdm(dataloader, disable=False, ascii='>='):\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            targets = targets.to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            predicts = self.Mymodel(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(predicts, targets)\n",
        "\n",
        "            # Backward pass and optimization step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update training statistics\n",
        "            train_loss += loss.item() * targets.size(0)\n",
        "            n_correct += (torch.argmax(predicts, dim=1) == targets).sum().item()\n",
        "            n_train += targets.size(0)\n",
        "\n",
        "        return train_loss / n_train, n_correct / n_train\n",
        "\n",
        "    def _test(self, dataloader, criterion):\n",
        "        # Testing function\n",
        "\n",
        "        test_loss, n_correct, n_test = 0, 0, 0\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        self.Mymodel.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in tqdm(dataloader, disable=True, ascii=' >='):\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                predicts = self.Mymodel(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(predicts, targets)\n",
        "\n",
        "                # Update testing statistics\n",
        "                test_loss += loss.item() * targets.size(0)\n",
        "                n_correct += (torch.argmax(predicts, dim=1) == targets).sum().item()\n",
        "                n_test += targets.size(0)\n",
        "\n",
        "        return test_loss / n_test, n_correct / n_test\n",
        "\n",
        "    def run(self):\n",
        "        # Main training and testing function\n",
        "\n",
        "        # Load train and test dataloaders\n",
        "        train_dataloader, test_dataloader = load_dataset(\n",
        "            tokenizer=self.tokenizer,\n",
        "            train_batch_size=self.train_batch_size,\n",
        "            test_batch_size=self.test_batch_size\n",
        "        )\n",
        "\n",
        "        # Get parameters for optimization (excluding frozen layers)\n",
        "        _params = filter(lambda x: x.requires_grad, self.Mymodel.parameters())\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.AdamW(_params, lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        # Lists for storing metrics across epochs\n",
        "        l_acc, l_trloss, l_teloss, l_epo = [], [], [], []\n",
        "\n",
        "        # Initialize best loss and best accuracy\n",
        "        best_loss, best_acc = 0, 0\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.num_epoch):\n",
        "            train_loss, train_acc = self._train(train_dataloader, criterion, optimizer)\n",
        "            test_loss, test_acc = self._test(test_dataloader, criterion)\n",
        "\n",
        "            # Append metrics to lists\n",
        "            l_epo.append(epoch)\n",
        "            l_acc.append(test_acc)\n",
        "            l_trloss.append(train_loss)\n",
        "            l_teloss.append(test_loss)\n",
        "\n",
        "            # Update best metrics\n",
        "            if test_acc > best_acc or (test_acc == best_acc and test_loss < best_loss):\n",
        "                best_acc, best_loss = test_acc, test_loss\n",
        "\n",
        "            # Print epoch statistics\n",
        "            print('{}/{} - {:.2f}%'.format(epoch + 1, self.num_epoch, 100 * (epoch + 1) / self.num_epoch))\n",
        "            print('[train] loss: {:.4f}, acc: {:.2f}'.format(train_loss, train_acc * 100))\n",
        "            print('[test] loss: {:.4f}, acc: {:.2f}'.format(test_loss, test_acc * 100))\n",
        "\n",
        "        # Print best metrics achieved during training\n",
        "        print('Best loss: {:.4f}, Best accuracy: {:.2f}'.format(best_loss, best_acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKQECbtqxwBK"
      },
      "source": [
        "# **Bert with Bi-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4oPjU-sxRs2",
        "outputId": "984fd7ff-0ce5-4acf-c218-b57440394b6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [07:56<00:00,  2.10it/s]\n",
            "100%|==========| 63/63 [00:34<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/10 - 10.00%\n",
            "[train] loss: 0.5118, acc: 79.97\n",
            "[test] loss: 0.4356, acc: 87.70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:04<00:00,  2.07it/s]\n",
            "100%|==========| 63/63 [00:38<00:00,  1.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/10 - 20.00%\n",
            "[train] loss: 0.4077, acc: 90.48\n",
            "[test] loss: 0.4392, acc: 86.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [07:54<00:00,  2.11it/s]\n",
            "100%|==========| 63/63 [00:39<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/10 - 30.00%\n",
            "[train] loss: 0.3778, acc: 93.40\n",
            "[test] loss: 0.4198, acc: 89.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [07:59<00:00,  2.09it/s]\n",
            "100%|==========| 63/63 [00:41<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/10 - 40.00%\n",
            "[train] loss: 0.3639, acc: 94.92\n",
            "[test] loss: 0.4413, acc: 86.80\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [07:58<00:00,  2.09it/s]\n",
            "100%|==========| 63/63 [00:33<00:00,  1.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/10 - 50.00%\n",
            "[train] loss: 0.3621, acc: 94.85\n",
            "[test] loss: 0.4215, acc: 88.50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:09<00:00,  2.04it/s]\n",
            "100%|==========| 63/63 [00:51<00:00,  1.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/10 - 60.00%\n",
            "[train] loss: 0.3495, acc: 96.33\n",
            "[test] loss: 0.4205, acc: 89.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:09<00:00,  2.04it/s]\n",
            "100%|==========| 63/63 [00:48<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/10 - 70.00%\n",
            "[train] loss: 0.3462, acc: 96.65\n",
            "[test] loss: 0.4262, acc: 88.40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:09<00:00,  2.04it/s]\n",
            "100%|==========| 63/63 [00:45<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/10 - 80.00%\n",
            "[train] loss: 0.3500, acc: 96.20\n",
            "[test] loss: 0.4495, acc: 85.80\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:01<00:00,  2.08it/s]\n",
            "100%|==========| 63/63 [00:40<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/10 - 90.00%\n",
            "[train] loss: 0.3412, acc: 97.08\n",
            "[test] loss: 0.4208, acc: 88.90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [07:56<00:00,  2.10it/s]\n",
            "100%|==========| 63/63 [00:39<00:00,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 - 100.00%\n",
            "[train] loss: 0.3356, acc: 97.70\n",
            "[test] loss: 0.4435, acc: 86.90\n",
            "best loss: 0.4198, best acc: 89.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating an instance of NLPMethod\n",
        "method = NLPMethod(method_name='bilstm', train_batch_size=4, test_batch_size=16, num_epoch=10, lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Running the training and testing process\n",
        "method.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Da58h788Lg",
        "outputId": "b3a4200e-d29e-4f5a-dd36-c93d32efdbe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [09:04<00:00,  1.84it/s]\n",
            "100%|==========| 63/63 [00:34<00:00,  1.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/10 - 10.00%\n",
            "[train] loss: 0.6054, acc: 71.43\n",
            "[test] loss: 0.5000, acc: 88.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:55<00:00,  1.87it/s]\n",
            "100%|==========| 63/63 [00:57<00:00,  1.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/10 - 20.00%\n",
            "[train] loss: 0.5253, acc: 85.95\n",
            "[test] loss: 0.5022, acc: 88.30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:39<00:00,  1.93it/s]\n",
            "100%|==========| 63/63 [00:33<00:00,  1.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/10 - 30.00%\n",
            "[train] loss: 0.5098, acc: 87.92\n",
            "[test] loss: 0.4891, acc: 89.90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:40<00:00,  1.92it/s]\n",
            "100%|==========| 63/63 [00:57<00:00,  1.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/10 - 40.00%\n",
            "[train] loss: 0.4945, acc: 90.25\n",
            "[test] loss: 0.5143, acc: 84.70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:27<00:00,  1.97it/s]\n",
            "100%|==========| 63/63 [00:38<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/10 - 50.00%\n",
            "[train] loss: 0.4834, acc: 91.70\n",
            "[test] loss: 0.4903, acc: 89.50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:28<00:00,  1.97it/s]\n",
            "100%|==========| 63/63 [00:36<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/10 - 60.00%\n",
            "[train] loss: 0.4782, acc: 92.22\n",
            "[test] loss: 0.4994, acc: 87.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:24<00:00,  1.98it/s]\n",
            "100%|==========| 63/63 [00:36<00:00,  1.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/10 - 70.00%\n",
            "[train] loss: 0.4806, acc: 91.30\n",
            "[test] loss: 0.4982, acc: 88.50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:26<00:00,  1.97it/s]\n",
            "100%|==========| 63/63 [00:32<00:00,  1.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/10 - 80.00%\n",
            "[train] loss: 0.4777, acc: 91.45\n",
            "[test] loss: 0.4971, acc: 87.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:26<00:00,  1.98it/s]\n",
            "100%|==========| 63/63 [00:35<00:00,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/10 - 90.00%\n",
            "[train] loss: 0.4688, acc: 92.75\n",
            "[test] loss: 0.4874, acc: 88.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [08:34<00:00,  1.94it/s]\n",
            "100%|==========| 63/63 [00:40<00:00,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 - 100.00%\n",
            "[train] loss: 0.4571, acc: 94.33\n",
            "[test] loss: 0.5224, acc: 81.90\n",
            "best loss: 0.4891, best acc: 89.90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating an instance of NLPMethod\n",
        "TC_method = NLPMethod(method_name='textcnn', train_batch_size=4, test_batch_size=16, num_epoch=10, lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Running the training and testing process\n",
        "TC_method.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGR9MQPr9B1x",
        "outputId": "0d8b6b68-5f94-46cc-bbc2-5e122e16f443"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [05:06<00:00,  3.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/10 - 10.00%\n",
            "[train] loss: 0.4947, acc: 80.38\n",
            "[test] loss: 0.4350, acc: 86.90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [04:21<00:00,  3.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/10 - 20.00%\n",
            "[train] loss: 0.3937, acc: 91.83\n",
            "[test] loss: 0.4154, acc: 89.50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [04:23<00:00,  3.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/10 - 30.00%\n",
            "[train] loss: 0.3770, acc: 93.45\n",
            "[test] loss: 0.4388, acc: 87.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [04:20<00:00,  3.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/10 - 40.00%\n",
            "[train] loss: 0.3610, acc: 95.15\n",
            "[test] loss: 0.4546, acc: 85.50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [04:20<00:00,  3.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/10 - 50.00%\n",
            "[train] loss: 0.3534, acc: 96.03\n",
            "[test] loss: 0.4487, acc: 86.20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|==========| 1000/1000 [04:20<00:00,  3.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/10 - 60.00%\n",
            "[train] loss: 0.3568, acc: 95.58\n",
            "[test] loss: 0.4262, acc: 88.60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|==>>>>>>>>| 268/1000 [01:08<02:44,  4.46it/s]"
          ]
        }
      ],
      "source": [
        "# Creating an instance of NLPMethod\n",
        "FN_method = NLPMethod(method_name='fnn', train_batch_size=4, test_batch_size=16, num_epoch=10, lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Running the training and testing process\n",
        "FN_method.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}